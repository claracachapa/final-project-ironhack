{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a8217e-0f23-471c-9aa1-11c5301eb269",
   "metadata": {},
   "source": [
    "# Loading and cleaning the raw dataset\n",
    "The original dataset used can be retrieved from MET museum open access GitHub repository: https://github.com/metmuseum/openaccess/tree/master "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f78777-bcdc-44ec-8916-9d4564f00458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "met_df = pd.read_csv('../data/raw/met_objects.txt', low_memory=False)\n",
    "met_df.columns = (met_df.columns.str.strip()\n",
    "                  .str.lower()                \n",
    "                  .str.replace(' ', '_')       \n",
    "                  .str.replace('[^a-z0-9_]', '')  \n",
    ")\n",
    "\n",
    "display(met_df.tail())\n",
    "print(met_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e7a57-36a7-4698-a95f-992b31bdb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "met_df = met_df.drop(columns = ['metadata_date','is_timeline_work', 'object_number', 'rights_and_reproduction',\n",
    "                               'object_wikidata_url', 'tags_aat_url', 'tags_wikidata_url','repository',\n",
    "                               'constituent_id'])\n",
    "met_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfebfc26-a996-463f-9ed2-d7505d5c03e5",
   "metadata": {},
   "source": [
    "# Fetching the missing values directly from the MET website\n",
    "There are multiple missing values in most columns. In this case, it is important to keep as many columns as possible with as much relevant information about the artworks for research purposes. Therefore, I chose to fetch the missing values directly from the item's url whenever it was possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00cea3-c4a4-48df-8eaf-95905c5cc7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process a single bin of rows with missing titles\n",
    "def fetch_titles_for_bin(bin_df, met_df):\n",
    "    for index, row in bin_df.iterrows():\n",
    "        url = row['link_resource']  # The URL column\n",
    "        try:\n",
    "            # Fetch the page\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Extract the title from the specific <span> element\n",
    "                title_element = soup.find('span', class_=\"artwork__title--text js-artwork__title--text\")\n",
    "                if title_element:\n",
    "                    met_df.at[index, 'title'] = title_element.text.strip()  # Update the original DataFrame\n",
    "                else:\n",
    "                    met_df.at[index, 'title'] = \"Title not found\"\n",
    "            else:\n",
    "                met_df.at[index, 'title'] = f\"Error {response.status_code}\"\n",
    "        except Exception as e:\n",
    "            met_df.at[index, 'title'] = f\"Error: {e}\"\n",
    "\n",
    "# Filter rows with missing titles\n",
    "missing_title_df = met_df[met_df['title'].isna()]\n",
    "\n",
    "# Split the rows with missing titles into bins of size 1000\n",
    "bins = [missing_title_df.iloc[i:i+1000] for i in range(0, len(missing_title_df), 1000)]\n",
    "\n",
    "# Process each bin one by one\n",
    "for i, bin_df in enumerate(bins):\n",
    "    print(f\"Processing bin {i+1}/{len(bins)}...\")\n",
    "    fetch_titles_for_bin(bin_df, met_df)  # Process the current bin\n",
    "    time.sleep(1)  # Add a delay between bins if needed (e.g., to avoid server overload)\n",
    "\n",
    "    # Optionally save progress to a file after each bin (recommended for large datasets)\n",
    "    met_df.to_csv('met_df_updated.csv', index=False)\n",
    "    print(f\"Bin {i+1} completed and progress saved.\")\n",
    "\n",
    "# Final message\n",
    "print(\"All bins processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some duplicated object numbers that need to be taken care of. \n",
    "# However, they don't always have the same values in every column, so this needs to be taken into account, ass to not lose any objects\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "general_df = pd.read_csv('../data/clean/met_df_updated.csv', low_memory=False)\n",
    "\n",
    "general_df = general_df.drop_duplicates()\n",
    "\n",
    "duplicate_counts = general_df['object_number'].value_counts()\n",
    "duplicates = duplicate_counts[duplicate_counts > 1]\n",
    "print(duplicates)\n",
    "print(general_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602281e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values in the 'title' column with 'Untitled'\n",
    "general_df['title'] = general_df['title'].fillna('Untitled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af9ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the types of the columns \n",
    "general_df['accession_year'] = pd.to_numeric(general_df['accessionyear'], errors='coerce')\n",
    "general_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897733b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_column = met_df['gallery_number']  # Replace 'column_name' with the actual name\n",
    "\n",
    "# Add it back to the updated dataset\n",
    "#general_df['gallery_number'] = missing_column\n",
    "\n",
    "# Move the column to the first position\n",
    "#column_name = 'gallery_number'  # Replace with the actual column name\n",
    "#columns = [col for col in general_df.columns if col != column_name]  # Exclude the column first\n",
    "#columns.insert(6, column_name)  # Insert the column at the 7th position (index 6)\n",
    "#general_df = general_df[columns]\n",
    "#general_df.drop(columns = ['is_timeline_work'])\n",
    "general_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30604821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the irrelevant columns\n",
    "general_df = general_df.drop(columns=['is_timeline_work', 'object_number', 'accessionyear'])\n",
    "\n",
    "# Drop rows where the department is 'The Cloisters'\n",
    "general_df = general_df.drop(general_df[general_df['department'] == 'The Cloisters'].index)\n",
    "\n",
    "# Check the shape of the DataFrame\n",
    "general_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c5ff0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that everything seems to be correct, let's overwrite the met_df_updated\n",
    "general_df.to_csv('../data/clean/met_df_updated.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
